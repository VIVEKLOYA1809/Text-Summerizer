{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtM88RE5_uOF"
      },
      "source": [
        "## importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTFXut-RcHth"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "from tensorflow.keras import backend as K\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed,Add,AdditiveAttention\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(32)\n",
        "rn.seed(12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Ln6wrijylE",
        "outputId": "7bec49b5-2028-4bba-b07c-51e62f1aa89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5DgO0w1XxPO"
      },
      "outputs": [],
      "source": [
        "text_data=pd.read_csv('/content/text_data.csv')\n",
        "text_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gJ-TwjV4DGq"
      },
      "outputs": [],
      "source": [
        "def conv_str(text):\n",
        "  new_text=''\n",
        "  try:\n",
        "     text = [str(item) for item in text.split()]\n",
        "  except:\n",
        "     text=str(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhcL_DVK4IWJ"
      },
      "outputs": [],
      "source": [
        "text_data['summary']=text_data['summary'].map(conv_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua3iaOe7etEa"
      },
      "outputs": [],
      "source": [
        "max_text_len = 500\n",
        "max_summary_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOjASdohi6L5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_validation,y_train,y_validatioin=train_test_split(np.array(text_data['article']),np.array(text_data['summary']),random_state=33 ,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF0T6FzTqRtA"
      },
      "source": [
        "### Considering rare words as unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ONVo1DxwjOuZ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5MkrsaWCnQx5"
      },
      "outputs": [],
      "source": [
        "thresh=2\n",
        "rare_word=[]\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    if(value<thresh):\n",
        "        rare_word.append(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmqgi9CcnWls",
        "outputId": "83ee10d8-88c5-4dc8-a034-d924c1da0b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39072\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['naoma', 'gianato', 'msha', 'illegitimacy', 'indignantly']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(rare_word))\n",
        "rare_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D47f_5I_o5Fi"
      },
      "outputs": [],
      "source": [
        "tokenrare=[]\n",
        "for i in range(len(rare_word)):\n",
        "    tokenrare.append('ukn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h6ykDgdco7g6"
      },
      "outputs": [],
      "source": [
        "dictionary_1 = dict(zip(rare_word,tokenrare))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2aF1svho7MU"
      },
      "outputs": [],
      "source": [
        "x_trunk=[]\n",
        "for i in x_train:\n",
        "    for word in i.split():\n",
        "        if word.lower() in dictionary_1:\n",
        "            i = i.replace(word, dictionary_1[word.lower()])\n",
        "    x_trunk.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOaB_Qw6pPyE"
      },
      "outputs": [],
      "source": [
        "x_tokenizer = Tokenizer(oov_token='ukn')\n",
        "x_tokenizer.fit_on_texts(list(x_trunk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MbdFS5bnBZa"
      },
      "outputs": [],
      "source": [
        "# Convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_trunk)\n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_validation)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  len(x_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyfMUhRdrEOM"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-g4d6Paja4c"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LutW4j8VC6L"
      },
      "outputs": [],
      "source": [
        "thresh=2\n",
        "rare_word=[]\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    if(value<thresh):\n",
        "        rare_word.append(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CrAoTCQlRyi"
      },
      "outputs": [],
      "source": [
        "print(len(rare_word))\n",
        "rare_word[3:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih87v7anG7Pv"
      },
      "outputs": [],
      "source": [
        "tokenrare=[]\n",
        "for i in range(len(rare_word)):\n",
        "    tokenrare.append('ukn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqPxITdTRSRI"
      },
      "outputs": [],
      "source": [
        "dictionary_1 = dict(zip(rare_word,tokenrare))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX6asWzzR9qY"
      },
      "outputs": [],
      "source": [
        "y_trunk=[]\n",
        "for i in y_train:\n",
        "    for word in i:\n",
        "        if word.lower() in dictionary_1:\n",
        "            i[i.index(word)] = dictionary_1[word.lower()]\n",
        "    y_trunk.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU-5ezxIQbH9"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer(oov_token='ukn')\n",
        "y_tokenizer.fit_on_texts(y_trunk)\n",
        "\n",
        "# Convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_trunk)\n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_validatioin)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# Size of vocabulary\n",
        "y_voc  =   len(y_tokenizer.word_index) +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVnhvGMNUGhw"
      },
      "outputs": [],
      "source": [
        "y_voc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63wwKajK_3_c"
      },
      "source": [
        "## loading glove vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ysgf0Qg4IHy"
      },
      "outputs": [],
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open(\"/content/drive/MyDrive/glove.42B.300d.txt\", encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNaW4yZu4IZT"
      },
      "outputs": [],
      "source": [
        "#Updating the dictionary with the pre-trained GloVe embeddings.\n",
        "embedding_matrix_x = np.zeros((x_voc+1 , 300))\n",
        "for word, index in x_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_x[index] = embedding_vector\n",
        "embedding_matrix_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxQolp7v4Ikm"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_y = np.zeros((y_voc+1, 300))\n",
        "\n",
        "for word, index in y_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_y[index] = embedding_vector\n",
        "embedding_matrix_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QELO7BGAATT"
      },
      "source": [
        "## Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aMe5THwE3Ub"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim,embedding_matrix_x, hidden_units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[embedding_matrix_x])\n",
        "        self.bi_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',dropout=0.08,recurrent_dropout=0.05))\n",
        "\n",
        "    def call(self, encoder_input,encoder_states):\n",
        "        # inputs: encoder_input = (batch_size, seq_length)\n",
        "        #         encoder_states = list[(batch_size, hidden_units),(batch_size, hidden_units)]\n",
        "\n",
        "        # embedding look-up layer\n",
        "        encoder_emb = self.embedding(encoder_input) # (batch_size,seq_length,embedding_dim)\n",
        "\n",
        "        # encoder_output = (batch_size,seq_length,hidden_units)\n",
        "        # encoder_states = (batch_size,hidden_units)\n",
        "        encoder_output, state_fwd, state_back = self.bi_gru(encoder_emb,initial_state=encoder_states)\n",
        "        encoder_states = [state_fwd,state_back]\n",
        "\n",
        "        return encoder_output, encoder_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQsdbfbAGOs"
      },
      "source": [
        "## Attention mechanism with coverage vector(wc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj7kEHEldWQq"
      },
      "outputs": [],
      "source": [
        "class additiveAttention(tf.keras.layers.AdditiveAttention):\n",
        "    def __init__(self, hidden_units,is_coverage=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wh = tf.keras.layers.Dense(hidden_units) # weight matrix for encoder hidden state\n",
        "        self.Ws = tf.keras.layers.Dense(hidden_units) # weight matrix for decoder state\n",
        "        self.wc = tf.keras.layers.Dense(1) # weight vector for coverage\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.coverage = is_coverage\n",
        "        if self.coverage is False:\n",
        "            self.wc.trainable = False\n",
        "\n",
        "    def call(self,keys):\n",
        "\n",
        "\n",
        "        value=keys[0]\n",
        "        query=keys[1]\n",
        "        ct=keys[2]\n",
        "\n",
        "\n",
        "        value = tf.expand_dims(value, 1) # (batch_size, 1, hidden_units)\n",
        "        ct = tf.expand_dims(ct, 1) # (batch_size, 1, seq_length)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                        self.Wh(query) +\n",
        "                        self.Ws(value) +\n",
        "\n",
        "                        self.wc(ct)\n",
        "                        ))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1) # (batch_size, seq_length, 1)\n",
        "        # only update coverage vector if coverage is enabled\n",
        "        ct = tf.squeeze(ct,1) # (batch_size, seq_length)\n",
        "        if self.coverage is True:\n",
        "            ct+=tf.squeeze(attention_weights)\n",
        "\n",
        "        context_vector = attention_weights * query # (batch_size, seq_length, hidden_units)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) # (batch_size, hidden_units)\n",
        "\n",
        "        return context_vector, attention_weights, ct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q5obBiwAUwF"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn41ICcufmQx"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim,embedding_matrix_y,hidden_units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,weights=[embedding_matrix_y])\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "        )\n",
        "        self.W1 = tf.keras.layers.Dense(hidden_units)\n",
        "        self.W2 = tf.keras.layers.Dense(vocab_size)\n",
        "        # Pointer Generator\n",
        "        # wh = tf.keras.layers.Dense(1)\n",
        "        # ws = tf.keras.layers.Dense(1)\n",
        "        # wx = tf.keras.layers.Dense(1)\n",
        "\n",
        "\n",
        "    def call(self, decoder_input, decoder_state, encoder_output,context_vector):\n",
        "        # inputs: decoder_input = (batch_size, 1)\n",
        "        #         decoder_state = (batch_size, hidden_units)\n",
        "        #         encoder_output = (batch_size,seq_length, hidden_units)\n",
        "        #         coverage_vector = (batch_size,seq_length)\n",
        "\n",
        "        # embedding look-up layer\n",
        "        decoder_emb = self.embedding(decoder_input) # (batch_size, seq_length, hidden_units)\n",
        "\n",
        "        # decoder_output = (batch_size,seq_length,hidden_units)\n",
        "        # decoder_state = (batch_size,hidden_units)\n",
        "        decoder_output , decoder_state = self.gru(decoder_emb,initial_state=decoder_state)\n",
        "\n",
        "        # concatenate context vector and decoder state\n",
        "        concat_vector = tf.concat([context_vector,decoder_state], axis=-1)\n",
        "        # reshape to 1d array\n",
        "        concat_vector = tf.reshape(concat_vector, (-1, concat_vector.shape[1]))\n",
        "        # create vocabulary distribution\n",
        "        p_vocab = tf.nn.log_softmax(self.W2(self.W1(concat_vector)))\n",
        "\n",
        "        # calculate p_gen\n",
        "        #p_gen = tf.nn.sigmoid(self.wh(context_vector)+self.ws(decoder_state)+self.wx(decoder_input))\n",
        "\n",
        "        return p_vocab, decoder_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI7QQ1dXf3Vp"
      },
      "outputs": [],
      "source": [
        "#Set Parameters\n",
        "input_vocab_size = x_voc+1\n",
        "output_vocab_size = y_voc +1\n",
        "#Encoding and decoding Embedding layer dimension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G_Pn_B_UE40"
      },
      "source": [
        "### Data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru6ht6CUgLCM"
      },
      "outputs": [],
      "source": [
        "def data_generator(X,y,BATCH_SIZE,shuffle=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(len(X)).batch(BATCH_SIZE,drop_remainder=True)\n",
        "    else:\n",
        "        dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vn-sFHfi4Nr"
      },
      "outputs": [],
      "source": [
        "body_seqs=x_tr\n",
        "target_seqs=y_tr\n",
        "\n",
        "body_seqs_val=x_val\n",
        "target_seqs_val=y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMtqprAigPRO"
      },
      "outputs": [],
      "source": [
        "train_dataset = data_generator(body_seqs,target_seqs,BATCH_SIZE=64,\n",
        "                       shuffle=True)\n",
        "val_dataset = data_generator(body_seqs_val,target_seqs_val,BATCH_SIZE=64,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "petoDFBYfrSX"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 300\n",
        "hidden_units = 128\n",
        "batch_size=64\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_dim,embedding_matrix_x, hidden_units)\n",
        "attention = additiveAttention(hidden_units,is_coverage=True)\n",
        "decoder = Decoder(output_vocab_size, embedding_dim,embedding_matrix_y,hidden_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBCZanu5f7KL"
      },
      "outputs": [],
      "source": [
        "encoder_input, decoder_target = next(iter(train_dataset))\n",
        "encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(2)]\n",
        "encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "decoder_state = encoder_states[0]\n",
        "coverage_vector = tf.zeros((64,encoder_input.shape[1]))\n",
        "decoder_input_t = decoder_target[:,0]\n",
        "context_vector, attention_weights, coverage_vector = attention([decoder_state,encoder_output,coverage_vector])\n",
        "p_vocab,decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTaNcKcEfrDg"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def nll_loss(p_vocab,target):\n",
        "    # apply a mask such that pad zeros do not affect the loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss = -p_vocab\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return loss\n",
        "\n",
        "def coverage_loss(attention_weights,coverage_vector,target):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    coverage_vector = tf.expand_dims(coverage_vector,axis=2)\n",
        "    ct_min = tf.reduce_min(tf.concat([attention_weights,coverage_vector],axis=2),axis=2)\n",
        "    cov_loss = tf.reduce_sum(ct_min,axis=1)\n",
        "    mask = tf.cast(mask, dtype=cov_loss.dtype)\n",
        "    cov_loss *= mask\n",
        "    return cov_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUSx533BCUK"
      },
      "source": [
        "### Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO-cgl0DUE5H"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EPseieJ8kCE"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(encoder_input, decoder_target):\n",
        "    \"\"\"Function which performs one training step (batch)\"\"\"\n",
        "    loss = tf.zeros(batch_size)\n",
        "    lambda_cov = 1\n",
        "    with tf.GradientTape() as tape:\n",
        "        # run body_sequence input through encoder\n",
        "        encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(2)]\n",
        "        encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "        # initialize decoder with encoder forward state\n",
        "        decoder_state = encoder_states[0] # !!!interpolate between forward and backward instead!!!\n",
        "        coverage_vector = tf.zeros((64,encoder_input.shape[1]))\n",
        "        # loop over each word in target sequence\n",
        "        for t in range(decoder_target.shape[1]-1):\n",
        "            # run decoder input through decoder and generate vocabulary distribution\n",
        "            decoder_input_t = decoder_target[:,t]\n",
        "            decoder_target_t = decoder_target[:,t+1]\n",
        "            # get attention scores\n",
        "            context_vector, attention_weights, coverage_vector = attention([decoder_state, encoder_output,coverage_vector])\n",
        "            # get vocabulary distribution for each batch at time t\n",
        "            p_vocab,decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "            # for each batch get the probability of the target word at time t+1\n",
        "            p_vocab_list = []\n",
        "            for i in range(len(decoder_target_t)):\n",
        "                p_vocab_list.append(p_vocab[i,decoder_target_t[i]])\n",
        "            p_vocab_target = tf.stack(p_vocab_list)\n",
        "            # calculate the loss at each time step t and add to current loss\n",
        "            loss += nll_loss(p_vocab_target,decoder_target_t) + lambda_cov*coverage_loss(attention_weights,coverage_vector,decoder_target_t)\n",
        "\n",
        "        # get the non-padded length of each sequence in the batch\n",
        "        seq_len_mask = tf.cast(tf.math.logical_not(tf.math.equal(decoder_target, 0)),tf.float32)\n",
        "        batch_seq_len = tf.reduce_sum(seq_len_mask,axis=1)\n",
        "\n",
        "        # get batch loss by dividing the loss of each batch by the target sequence length and mean\n",
        "        batch_loss = tf.reduce_mean(loss/batch_seq_len)\n",
        "\n",
        "    # update trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(batch_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl6sGkVqUE5K"
      },
      "source": [
        "### Validation_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCmyfVhjKVkf"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def val_step(encoder_input, decoder_target):\n",
        "    loss = tf.zeros(batch_size)\n",
        "    lambda_cov = 1\n",
        "\n",
        "    encoder_init_states = [tf.zeros((batch_size, encoder.hidden_units)) for i in range(2)]\n",
        "    encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "    decoder_state = encoder_states[0]\n",
        "    coverage_vector = tf.zeros((64,encoder_input.shape[1]))\n",
        "\n",
        "\n",
        "    for t in range(decoder_target.shape[1]-1):\n",
        "            # run decoder input through decoder and generate vocabulary distribution\n",
        "        decoder_input_t = decoder_target[:,t]\n",
        "        decoder_target_t = decoder_target[:,t+1]\n",
        "            # get attention scores\n",
        "        context_vector, attention_weights, coverage_vector = attention([decoder_state, encoder_output,coverage_vector])\n",
        "            # get vocabulary distribution for each batch at time t\n",
        "        p_vocab,decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "            # for each batch get the probability of the target word at time t+1\n",
        "        p_vocab_list = []\n",
        "\n",
        "        for i in range(len(decoder_target_t)):\n",
        "            p_vocab_list.append(p_vocab[i,decoder_target_t[i]])\n",
        "\n",
        "        p_vocab_target = tf.stack(p_vocab_list)\n",
        "            # calculate the loss at each time step t and add to current loss\n",
        "        loss += nll_loss(p_vocab_target,decoder_target_t) + lambda_cov*coverage_loss(attention_weights,coverage_vector,decoder_target_t)\n",
        "\n",
        "        # get the non-padded length of each sequence in the batch\n",
        "\n",
        "    seq_len_mask = tf.cast(tf.math.logical_not(tf.math.equal(decoder_target, 0)),tf.float32)\n",
        "    batch_seq_len = tf.reduce_sum(seq_len_mask,axis=1)\n",
        "\n",
        "        # get batch loss by dividing the loss of each batch by the target sequence length and mean\n",
        "    val_batch_loss = tf.reduce_mean(loss/batch_seq_len)\n",
        "\n",
        "\n",
        "    return val_batch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogKYe2bbE3Vi"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ancI3Aj4Agrt"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ltdmssaq50i"
      },
      "outputs": [],
      "source": [
        "def decode_seq(encoder_input):\n",
        "    \"\"\"Function which returns a summary by always picking the highest probability option conditioned on the previous word\"\"\"\n",
        "    # run body_sequence through encoder\n",
        "    encoder_init_states = [tf.zeros((1, encoder.hidden_units)) for i in range(2)]\n",
        "    encoder_output, encoder_states = encoder(encoder_input,encoder_init_states)\n",
        "    # initialize decoder with encoder forward state\n",
        "    decoder_state = encoder_states[0]\n",
        "\n",
        "    decoder_input_t =  tf.ones(1)*target_word_index['start'] # initialize with start token\n",
        "    summary = [target_word_index['start']]\n",
        "    coverage_vector = tf.zeros((1,encoder_input.shape[1]))\n",
        "    while decoder_input_t[0].numpy()!=target_word_index['end'] and len(summary)<max_summary_len: # as long as decoder input is different from end token continue\n",
        "        context_vector, attention_weights, coverage_vector = attention([decoder_state, encoder_output,coverage_vector])\n",
        "        p_vocab, decoder_state = decoder(tf.expand_dims(decoder_input_t,1),decoder_state,encoder_output,context_vector)\n",
        "        decoder_input_t = tf.argmax(p_vocab,axis=1)\n",
        "        decoder_word_idx = int(decoder_input_t[0].numpy())\n",
        "        summary.append(decoder_word_idx)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F14s5XNlMp3R"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OutJrk06NdjR"
      },
      "outputs": [],
      "source": [
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2summary(input_seq,ukn_token):\n",
        "\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "            if i==target_word_index['ukn']:\n",
        "\n",
        "                newString=newString+ukn_token+' '\n",
        "            else:\n",
        "\n",
        "                newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "def search(list, platform):\n",
        "    for i in range(len(list)):\n",
        "        if list[i] == platform:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRKn4wnCzee2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for i in range(31,50):\n",
        "    encoder_input_sum = tf.expand_dims(x_val[i],0)\n",
        "    summary = decode_seq(encoder_input_sum)\n",
        "\n",
        "    k= seq2text(x_val[i])\n",
        "    k=re.sub('[^a-z]+', ' ', k)\n",
        "    result = text_to_word_sequence(k)\n",
        "\n",
        "\n",
        "    if search(result, 'ukn'):\n",
        "        idx=result.index('ukn')\n",
        "\n",
        "        input_org = re.sub('[^a-z]+',' ', x_validation[i])\n",
        "        input_org = text_to_word_sequence(input_org)\n",
        "        ukn_token = input_org[idx]\n",
        "\n",
        "    else:\n",
        "        ukn_token='ukn'\n",
        "\n",
        "    print(\"Original summary:\",y_validation[i])\n",
        "\n",
        "    print(\"Predicted summary:\",seq2summary(summary, ukn_token))\n",
        "\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}